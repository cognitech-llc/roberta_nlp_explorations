# Exploration of NLP Capabilities of BERT

### Background

One of the biggest challenges in natural language processing (NLP) is the shortage of training data. Because NLP is a diversified field with many distinct tasks, most task-specific datasets contain only a few thousand or a few hundred thousand human-labeled training examples. However, modern deep learning-based NLP models see benefits from much larger amounts of data, improving when trained on millions, or billions, of annotated training examples. To help close this gap in data, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of unannotated text on the web (known as pre-training). The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch.

More can be found on the [Google blog post for BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).

### Purpose

We aim to explore the capabilities of BERT and also acclimate ourselves to the nuances of developing using BERT so that we can potentially create a product with BERT as the engine.

### Findings

TBD